{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aadbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Logistics Supply Chain Delay Prediction¶By Jack Motta¶Table of Contents¶\n",
    "Introduction\n",
    "Background\n",
    "Methods\n",
    "Results\n",
    "Discussion\n",
    "Conclusion\n",
    "References\n",
    "Data Sources\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Introduction¶Project Overview¶For this machine learning project, I have chosen to focus on the logistics and supply chain industry, specifically predicting delivery outcomes—whether an order will arrive early, on time, or be delayed. I am particularly interested in this area because supply chain efficiency is a critical component of operational success for businesses, and delays in logistics can have significant financial and customer satisfaction consequences. With the increased reliance on e-commerce and global distribution networks, developing models that can proactively forecast potential delays can provide tremendous value to companies and customers alike.\n",
    "The primary dataset for this project is a comprehensive logistics and supply chain dataset obtained from Kaggle. It contains 15,549 records and 41 features, capturing detailed information about customer orders, products, geographic regions, shipping modes, and delivery outcomes. The data spans from 2014 to 2018, providing a rich historical context for analyzing shipping performance across different time periods and regions. In addition to this, I have identified a secondary dataset from the U.S. Energy Information Administration that includes weekly U.S. diesel prices from 1993 through 2025. By integrating fuel cost data with the supply chain records, I hope to explore whether fluctuations in diesel prices correlate with shipping delays or timing.\n",
    "The supply chain dataset includes both numerical and categorical variables that cover various aspects of each order. These include sales and profit metrics, customer and order location data, product and shipping information, and a target variable labeled as label, which indicates whether an order was delivered early (-1), on time (0), or delayed (1). This target variable serves as the foundation for the supervised learning model I will build.\n",
    "The problem I aim to solve with this dataset is a multi-class classification task: predicting delivery outcomes based on pre-shipping information. My goal is to uncover which factors—such as shipping mode, region, order characteristics, and possibly external data like diesel prices—have the strongest influence on whether an order arrives late, early, or on time. Accurately predicting these outcomes can help businesses make informed decisions to improve logistical planning, allocate resources more efficiently, and ultimately enhance customer satisfaction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Background¶Importance of the Problem & Background Knowledge¶Predicting delivery outcomes—whether an order arrives early, on time, or late—is a critical issue in modern supply chain management. Timely deliveries are essential not only for customer satisfaction but also for reducing operational costs, maintaining efficient inventory systems, and ensuring smooth business workflows. As global supply chains become more complex, the ability to anticipate delivery timing using predictive models has gained significant strategic value.\n",
    "Delivery performance is influenced by various factors, with lead time variability being particularly impactful. Inconsistent lead times increase uncertainty and can disrupt downstream logistics operations. Organizations with better control over lead time variability tend to perform more efficiently and manage risk more effectively (Rokoss et al., 2024). Transportation-related disruptions also significantly influence delivery outcomes. External factors such as traffic congestion, extreme weather events, and fluctuating fuel prices can cause delays even in otherwise stable supply chains (Ivanov & Dolgui, 2020). These disruptions not only increase delivery time but also reduce the reliability of shipping estimates, which can impact service-level agreements and customer expectations. Another major theme in this area is supply chain resilience—the capacity of a logistics system to adapt to unforeseen disruptions and recover quickly. Predictive analytics, including machine learning, play an essential role in building this resilience by helping organizations anticipate problems and act proactively (Gabellini et al., 2024). Machine learning models are particularly powerful tools for this kind of predictive work. When trained on large, diverse datasets that include historical delivery information and external variables such as diesel prices, these models can identify patterns and generate more accurate forecasts. Recent research shows that machine learning methods can improve decision-making across the supply chain, from demand forecasting to final-mile delivery prediction (Pazoki et al., 2023).\n",
    "Overall, the ability to predict delivery timing is a high-impact challenge at the intersection of logistics, transportation, and data science. By applying machine learning to real-world supply chain data, this project contributes to a vital area of research and practice.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe1ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 1: Data Dictionary\n",
    "\n",
    "\n",
    "\n",
    "Original Column (Relabeled Column)\n",
    "Type\n",
    "Description\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "payment_type\n",
    "Categorical\n",
    "Method of payment used for the order.\n",
    "\n",
    "\n",
    "profit_per_order (pre_discount_profit_per_order)\n",
    "Numerical\n",
    "Net income generated from each order before discount.\n",
    "\n",
    "\n",
    "sales_per_customer (total_sales_per_customer)\n",
    "Numerical\n",
    "Total revenue attributed to a single customer.\n",
    "\n",
    "\n",
    "category_id\n",
    "Numerical\n",
    "Numeric identifier for the product category.\n",
    "\n",
    "\n",
    "category_name\n",
    "Text\n",
    "Name of the category to which the product belongs.\n",
    "\n",
    "\n",
    "customer_city (customer_purchase_city)\n",
    "Categorical\n",
    "City where the customer made the purchase.\n",
    "\n",
    "\n",
    "customer_country (customer_purchase_country)\n",
    "Categorical\n",
    "Country where the customer made the purchase.\n",
    "\n",
    "\n",
    "customer_id\n",
    "Numerical\n",
    "Unique identifier for the customer.\n",
    "\n",
    "\n",
    "customer_segment\n",
    "Categorical\n",
    "Classification of customer type (e.g., Consumer, Corporate).\n",
    "\n",
    "\n",
    "customer_state (store_location_state)\n",
    "Categorical\n",
    "State in which the store processing the order is located.\n",
    "\n",
    "\n",
    "customer_zipcode\n",
    "Text\n",
    "Zip code of the customer's location.\n",
    "\n",
    "\n",
    "department_id\n",
    "Numerical\n",
    "Numeric ID for the store department.\n",
    "\n",
    "\n",
    "department_name\n",
    "Text\n",
    "Name of the department in the store.\n",
    "\n",
    "\n",
    "latitude\n",
    "Numerical\n",
    "Geographic latitude of the store location.\n",
    "\n",
    "\n",
    "longitude\n",
    "Numerical\n",
    "Geographic longitude of the store location.\n",
    "\n",
    "\n",
    "market\n",
    "Categorical\n",
    "Broad geographic market where the order is being delivered.\n",
    "\n",
    "\n",
    "order_city (order_delivery_city)\n",
    "Categorical\n",
    "City to which the order is being shipped.\n",
    "\n",
    "\n",
    "order_country (order_delivery_country)\n",
    "Categorical\n",
    "Country where the order is delivered.\n",
    "\n",
    "\n",
    "order_customer_id\n",
    "Numerical\n",
    "ID associated with the customer's order.\n",
    "\n",
    "\n",
    "order_date\n",
    "Datetime\n",
    "Date when the order was placed.\n",
    "\n",
    "\n",
    "order_id\n",
    "Numerical\n",
    "Unique code assigned to each order.\n",
    "\n",
    "\n",
    "order_item_cardprod_id\n",
    "Numerical\n",
    "Product identifier scanned via RFID.\n",
    "\n",
    "\n",
    "order_item_discount (item_discount_amount)\n",
    "Numerical\n",
    "Total discount amount applied to an item.\n",
    "\n",
    "\n",
    "order_item_discount_rate (item_discount_rate)\n",
    "Numerical\n",
    "Discount as a percentage of the item’s price.\n",
    "\n",
    "\n",
    "order_item_id\n",
    "Numerical\n",
    "Identifier for individual items in the order.\n",
    "\n",
    "\n",
    "order_item_product_price (item_price_before_discount)\n",
    "Numerical\n",
    "Original price of the product before any discounts.\n",
    "\n",
    "\n",
    "order_item_profit_ratio (item_profit_margin)\n",
    "Numerical\n",
    "Profit margin for each item in the order.\n",
    "\n",
    "\n",
    "order_item_quantity\n",
    "Numerical\n",
    "Number of units purchased per item.\n",
    "\n",
    "\n",
    "sales (total_sales)\n",
    "Numerical\n",
    "Total amount in sales generated.\n",
    "\n",
    "\n",
    "order_item_total_amount (item_total_after_discount)\n",
    "Numerical\n",
    "Overall amount charged for the item, including quantity and discounts.\n",
    "\n",
    "\n",
    "order_profit_per_order (post_discount_profit_per_order)\n",
    "Numerical\n",
    "Profit earned on a per-order basis after discount.\n",
    "\n",
    "\n",
    "order_region\n",
    "Categorical\n",
    "Specific region where the order is shipped (e.g., Southeast Asia, Europe).\n",
    "\n",
    "\n",
    "order_state (order_delivery_state)\n",
    "Categorical\n",
    "State or province of the delivery region.\n",
    "\n",
    "\n",
    "order_status (order_current_status)\n",
    "Categorical\n",
    "Current status of the order (e.g., COMPLETE, CANCELED, ON_HOLD).\n",
    "\n",
    "\n",
    "product_card_id\n",
    "Numerical\n",
    "Identifier for the product's card code.\n",
    "\n",
    "\n",
    "product_category_id\n",
    "Numerical\n",
    "Category code linked to the product type.\n",
    "\n",
    "\n",
    "product_name\n",
    "Text\n",
    "Full name or description of the product.\n",
    "\n",
    "\n",
    "product_price (product_retail_price)\n",
    "Numerical\n",
    "Retail price of the product.\n",
    "\n",
    "\n",
    "shipping_date\n",
    "Datetime\n",
    "Timestamp for when the item was shipped.\n",
    "\n",
    "\n",
    "shipping_mode\n",
    "Categorical\n",
    "Type of shipping selected (e.g., Standard Class, Same Day).\n",
    "\n",
    "\n",
    "label\n",
    "Categorical\n",
    "Delivery outcome: -1 for early, 0 for on time, 1 for delayed.\n",
    "\n",
    "\n",
    "diesel_price\n",
    "Numerical\n",
    "Weekly diesel price.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Import Libraries and Datasets¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [6]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, FunctionTransformer, LabelEncoder, PowerTransformer, label_binarize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, ConfusionMatrixDisplay, auc\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [7]:\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Logistics Supply Chain Data Set\n",
    "delivery = pd.read_csv('Logistics.csv')\n",
    "print('\\nFirst 5 Observations of the Delivery Dataset:')\n",
    "display(delivery.head())\n",
    "\n",
    "# Diesel Prices Supplemental Data\n",
    "fuel = pd.read_excel('Diesel_Prices.xlsx', sheet_name='Data 1', skiprows=2)\n",
    "fuel.columns = ['date', 'diesel_price']\n",
    "print('\\nFirst 5 Observations of the Weekly Diesel Prices Dataset:')\n",
    "display(fuel.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Methods¶Project Goal¶The primary goal of this project is to build a machine learning model that can predict delivery outcomes—specifically, whether an order will arrive early (-1), on time (0), or delayed (1). The classification will be based on features available prior to shipment, including product information, order metadata, shipping method, and geographic and customer-related attributes. This classification task is important because predicting delivery outcomes before an item is shipped allows businesses to proactively address potential issues in logistics. By identifying patterns that lead to delays or early deliveries, companies can optimize shipping decisions, adjust resource allocation, and improve customer satisfaction. In a supply chain environment where timeliness is crucial, even small improvements in delivery reliability can lead to significant operational and financial gains.\n",
    "We plan to build and compare a Softmax Regression model, multiclass Random Forest and XGBoost models, and a binary version of whichever performs best among those three.\n",
    "Data Description¶The main dataset used for this project consists of 15,549 rows and 41 columns, and it was obtained from Kaggle. The data includes information on customer orders placed between 2015 and 2018. Each row represents an item in an order, and the dataset contains both numerical and categorical features describing the order, the product, the customer, and the shipping process. There are 17 object, 1 integer, and 23 float variables before merging.\n",
    "The target variable for this classification problem is:\n",
    "\n",
    "label: Indicates the delivery outcome with three possible values:\n",
    "-1: Order arrived earlier than expected\n",
    "0: Order arrived on time\n",
    "1: Order was delayed\n",
    "\n",
    "\n",
    "\n",
    "However, we will eventually label encode these to where the values are only non-negative by increasing each by 1. Making it:\n",
    "\n",
    "0: Order arrived earlier than expected\n",
    "1: Order arrived on time\n",
    "2: Order was delayed\n",
    "\n",
    "Key predictor variables include:\n",
    "\n",
    "shipping_mode: The method of shipping selected (e.g., Standard Class, Same Day).\n",
    "order_region and order_country: Geographic indicators of where the order is being delivered.\n",
    "order_item_product_price, order_item_discount, order_item_quantity: Financial and product-specific variables.\n",
    "order_date and shipping_date: Timestamps that may help measure lead time or seasonal effects.\n",
    "customer_segment: Type of customer (e.g., Consumer, Corporate, Home Office).\n",
    "market: Broad geographic market classification (e.g., USCA, LATAM, Europe).\n",
    "sales_per_customer and profit_per_order: Performance metrics that may correlate with order priority or shipping decisions.\n",
    "\n",
    "Additionally, there appears to be an error in the original data dictionary provided on Kaggle, as the order_state will often be a state in the US, but the order_country and order_city won't be from the United States. This is likely because in this supply chain structure, it's common for orders to be registered at a store or fulfillment center located in one region (e.g., the United States) while the actual delivery destination is in another country (e.g., a LATAM region like Brazil or Mexico).\n",
    "Additionally, I plan to integrate a secondary dataset that includes weekly U.S. diesel prices from 1993 to 2025, obtained from the U.S. Energy Information Administration. For orders delivered within the U.S., I will join diesel price data based on the week of the shipping date to explore whether fuel costs impact delivery reliability.\n",
    "All variables have been reviewed and cleaned to ensure appropriate formatting, consistency in data types, and readiness for modeling. Categorical variables will be encoded, and numerical variables will be standardized or normalized as necessary during preprocessing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3650242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Merging Datasets¶To start, we will merge (left join) the supply chain data with the diesel prices, matching shipping date to the date of the diesel price by creating year-week formatted date for each dataset, then dropping it after merging.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [10]:\n",
    "\n",
    "\n",
    "# Convert dates to datetime type\n",
    "delivery['shipping_date'] = pd.to_datetime(delivery['shipping_date'], utc=True)\n",
    "delivery['order_date'] = pd.to_datetime(delivery['order_date'], utc=True)\n",
    "fuel['date'] = pd.to_datetime(fuel['date'])\n",
    "\n",
    "# Create year-week columns for joining purposes\n",
    "delivery['year_week'] = delivery['shipping_date'].dt.strftime('%Y-%U')\n",
    "fuel['date'] = fuel['date'].dt.strftime('%Y-%U')\n",
    "\n",
    "# Left join with the diesel prices df\n",
    "merged_df = delivery.merge(fuel, how='left', left_on='year_week', right_on='date')\n",
    "merged_df = merged_df.drop(columns=['year_week', 'date'])\n",
    "merged_df = merged_df.sort_values('order_date') \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Exploratory Data Analysis¶To get a better understanding of the data, we will plot some visualizations as well as some summary statistics below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [12]:\n",
    "\n",
    "\n",
    "merged_df['label_display'] = merged_df['label'].map({-1: 'Early', 0: 'On Time', 1: 'Delayed'})\n",
    "id_cols = ['category_id', 'customer_id', 'customer_zipcode',\n",
    "           'department_id', 'order_customer_id', 'order_id', \n",
    "           'order_item_cardprod_id', 'order_item_id', 'product_card_id',\n",
    "           'product_category_id', 'label', 'label_display']\n",
    "print('Table 2: Summary Statistics of Numerical Columns')\n",
    "display(merged_df.drop(columns=id_cols, axis=1).describe())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [13]:\n",
    "\n",
    "\n",
    "# Plot Figures 1-4 in 2x2 grid\n",
    "fig, axes = plt.subplots(2, 2, figsize=(17, 9))\n",
    "fig.suptitle(\"Figures 1-4: Summary Visualizations\", fontsize=16, y=0.95)\n",
    "\n",
    "# Figure 1: Delivery Outcome\n",
    "sns.countplot(data=merged_df, x='label_display', hue='label_display', ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Figure 1: Bar Plot of Delivery Outcome\")\n",
    "axes[0, 0].set_xlabel(\"Delivery Outcome\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "if axes[0, 0].get_legend():\n",
    "    axes[0, 0].get_legend().remove()\n",
    "\n",
    "# Figure 2: Shipping Mode By Delivery Outcome\n",
    "sns.countplot(data=merged_df, x='shipping_mode', hue='label_display', ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Figure 2: Bar Plot of Shipping Mode By Delivery Outcome\")\n",
    "axes[1, 0].set_xlabel(\"Shipping Mode\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "axes[1, 0].tick_params(axis='x')\n",
    "axes[1, 0].legend(title='Delivery Status')\n",
    "\n",
    "# Figure 3: Skewness of Numerical Features\n",
    "numerics = merged_df.drop(columns=id_cols).select_dtypes(include='number')\n",
    "skew_df = numerics.apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "sns.barplot(x=skew_df.values, y=skew_df.index, ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Figure 3: Bar Plot of Skewness of Numerical Features\")\n",
    "axes[0, 1].set_xlabel(\"Skewness\")\n",
    "axes[0, 1].set_ylabel(\"Feature\")\n",
    "\n",
    "# Figure 4: Destination Market By Delivery Status\n",
    "sns.countplot(data=merged_df, x='market', hue='label_display', ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Figure 4: Bar Plot of Destination Market By Delivery Status\")\n",
    "axes[1, 1].set_xlabel(\"Market\")\n",
    "axes[1, 1].set_ylabel(\"Count\")\n",
    "axes[1, 1].legend(title='Delivery Status')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Figure 5: Store Location By Delivery Status\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "fig.suptitle(\"Figure 5: Bar Plot of Store Location By Delivery Status\", fontsize=16)\n",
    "sns.countplot(data=merged_df, x='customer_state', hue='label_display', ax=ax)\n",
    "ax.set_xlabel(\"Store Location\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.legend(title='Delivery Status')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 6: Correlation Heatmap Plot\n",
    "fig, ax = plt.subplots(figsize=(17, 6))\n",
    "fig.suptitle(\"Figure 6: Correlation Heatmap Plot\", fontsize=16)\n",
    "sns.heatmap(merged_df.drop(columns=id_cols).select_dtypes(include='number').corr(),\n",
    "            annot=True, cmap='coolwarm', ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [14]:\n",
    "\n",
    "\n",
    "# Count the occurrences of each class in 'label_display'\n",
    "label_counts = merged_df['label_display'].value_counts()\n",
    "\n",
    "# Identify majority and minority class sizes\n",
    "majority_class = label_counts.max()\n",
    "minority_class = label_counts.min()\n",
    "imbalance_ratio = round(minority_class / majority_class, 3)\n",
    "\n",
    "# Create DataFrame with imbalance ratio info\n",
    "class_distribution = pd.DataFrame({\n",
    "    'Count': label_counts\n",
    "})\n",
    "class_distribution['Imbalance vs Majority'] = round(label_counts / majority_class, 3)\n",
    "\n",
    "print(class_distribution)\n",
    "print(f\"\\nOverall Imbalance Ratio (Minority / Majority): {imbalance_ratio}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Figure 1 reveals a moderate class imbalance in the target variable, with delayed deliveries as the majority class. The smallest class, on-time deliveries, represents 33.7% of the delayed class. While this imbalance is notable, it remains within an acceptable range and does not justify the use of SMOTE or other resampling techniques, which could compromise model generalizability and real-world applicability. The overrepresentation of delayed deliveries may reflect underlying operational inefficiencies which we should keep.\n",
    "The bar plot shown in Figure 2, illustrates the count of obsevations for each shipping mode by the delivery status. From it, we can deduce that whether a delivery will be early or not depends on the time alotted before it will be considered on-time or delayed, as the standard class which is the slowest type of shipping has the most amount of early arrivals by a significant amount. This reflects the more lenient time window given for orders with standard class shipping. Therefore, the shipping mode may prove to be vital in helping predict whether an order will be delayed, on time, or early.\n",
    "Figure 3 shows that there exists some non-insignifcant skewness in some of the numeric features, most prominently the profit per order, profit per order after discount (order_profit_per_order), and the order item discount. We should likely apply a log transformation on all of these features asides from the item quantity, discount rate, diesel price, latitude, and longitude. While not having skewness isn't an assumption for logistic regression or more complex ML models, it can hinder performance in logistic regression models at the very least and should be addressed.\n",
    "In Figure 4 and Figure 5, we can see the frequency of where orders are delivered from and to, with Figure 4 showing a bar plot of where most of the deliveries come from and help indicate where this supply chain company is located in, relatively speaking. We can see they primarily work in Puerto Rico, California, and New York along with a few other states in the US. However, Figure 5 shows that most of their deliveries are to Europe and Latin America. This suggests the company is structured where orders are registered through U.S. stores or fulfillment centers but delivered internationally, indicating a centralized processing hub serving cross-border customers. This helps explain why there are so many delays in the dataset due to the distance of the deliveries being international rather than local.\n",
    "Another note to make is there appears to be an invalid value in Figure 4, as there is a store location (customer_state) listed as '91732', it is very likely a zip code and we can determine the state of that zip code and replace it with the actual state abbreviation. In this case, the zip code is in California (CA). We will perform this during the data preprocessing.\n",
    "The correlation heatmap in Figure 6 helps showcase which numerical features are correlated with each other and we can extract a few useful insights from it. The most important one is there being multicollinearity among some of the features, particularly ones regarding the price and discount as there is an original price as well as the price after applying the discount, which should lead to it being correlated. Because logistic regression assumes no multicollinearity and we are focused on prediction, we will apply an L2 penalty in logistic regression, shrinking irrelevant or correlated predictors close to zero but not eliminating it entirely as some of these features may still be informative.\n",
    "We will now begin preprocessing the data set for modeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f7b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Preprocessing¶First we will begin by extracting information from the date of the order and shipping, however, we need to check if there are any invalid rows where the shipping occurs before the order is placed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe817fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [17]:\n",
    "\n",
    "\n",
    "print(\"Invalid rows:\", (merged_df['shipping_date'] < merged_df['order_date']).sum())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43962a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There is in fact about 5941 rows that are invalid, making this appear systematic. Since this is a good chunk of data to lose if we were to drop them, we will instead create a new column to flag a row as 1 if invalid, otherwise 0 and set the days between the two dates column to be NaN if that's the case and allow the median imputation to handle it. Now, we will extract the rest of the info we can from the dates.\n",
    "We will also feature engineer several features using the order date and shipping date. These include:\n",
    "\n",
    "Day of the week when order was placed\n",
    "Month when order was placed\n",
    "Year order was placed\n",
    "Whether the order was placed on a weekend or not\n",
    "Days between the order and shipping date\n",
    "An indicator of whether the date flow is invalid (where shipping date is before order date by mistake)\n",
    "\n",
    "Finally we relabeled our columns so when we extract the features for feature importance it's easier to distinguish which columns have to do with the discount and which have to do without the discount.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fca9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [19]:\n",
    "\n",
    "\n",
    "merged_df['order_dayofweek'] = merged_df['order_date'].dt.dayofweek\n",
    "merged_df['order_month'] = merged_df['order_date'].dt.month\n",
    "merged_df['order_year'] = merged_df['order_date'].dt.year\n",
    "merged_df['is_weekend'] = merged_df['order_dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "merged_df['days_between_order_and_ship'] = (merged_df['shipping_date'] - merged_df['order_date']).dt.days\n",
    "merged_df['invalid_date_flow'] = (merged_df['shipping_date'] < merged_df['order_date']).astype(int)\n",
    "merged_df.loc[merged_df['days_between_order_and_ship'] < 0, 'days_between_order_and_ship'] = np.nan\n",
    "\n",
    "# Relabeling\n",
    "column_relabel = {\n",
    "    'profit_per_order': 'pre_discount_profit_per_order',\n",
    "    'order_profit_per_order': 'post_discount_profit_per_order',\n",
    "    'sales_per_customer': 'total_sales_per_customer',\n",
    "    'order_item_product_price': 'item_price_before_discount',\n",
    "    'order_item_total_amount': 'item_total_after_discount',\n",
    "    'product_price': 'product_retail_price',\n",
    "    'sales': 'total_sales',\n",
    "    'order_item_discount': 'item_discount_amount',\n",
    "    'order_item_discount_rate': 'item_discount_rate',\n",
    "    'order_item_profit_ratio': 'item_profit_margin',\n",
    "    'order_status': 'order_current_status',\n",
    "    'order_state': 'order_delivery_state',\n",
    "    'order_country': 'order_delivery_country',\n",
    "    'order_city': 'order_delivery_city',\n",
    "    'customer_country': 'customer_purchase_country',\n",
    "    'customer_city': 'customer_purchase_city',\n",
    "    'customer_state': 'store_location_state',\n",
    "}\n",
    "\n",
    "merged_df = merged_df.rename(columns=column_relabel)\n",
    "print('Column Names After Relabeling:\\n' + str(merged_df.columns))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220358a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we have successfully relabeled the column names for clarity, let's check to see which columns specifically have missing values and then plot the distribution of those columns to determine whether we should use mean or median imputation based on the skewness.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e511ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [21]:\n",
    "\n",
    "\n",
    "na_counts = merged_df.isnull().sum()\n",
    "print(f'Sum of NAs in Columns with at least 1 NA:\\n{na_counts[na_counts > 0]}') # Show sum of NAs for columns with at least 1 NA\n",
    "\n",
    "print()\n",
    "# Histogram 1: Days Between Order and Ship\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "axes[0].hist(merged_df['days_between_order_and_ship'], bins=20, edgecolor='black')\n",
    "axes[0].set_title(\"Days Between Order and Shipping Date\")\n",
    "axes[0].set_xlabel(\"Days Between Order and Ship\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Histogram 2: Diesel Price\n",
    "axes[1].hist(merged_df['diesel_price'], bins=10, edgecolor='black')\n",
    "axes[1].set_title(\"Diesel Price Distribution\")\n",
    "axes[1].set_xlabel(\"Diesel Price\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"Figure 7 & 8: Histograms of Order Processing and Diesel Prices\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have 66 missing values in diesel price, we will impute those values with the median price. We will also replace the invalid value in customer_state of '91732' with 'CA', since the zipcode is in California according to Google. Next we will log transform skewed predictors that have no negative values with a small added constant, onehot encode categorical predictors, ordinal encode shipping_mode, and then finally standardize all remaining numeric predictors. To do the log transformation with an added constant we will create a simple function called safe_log1p(). In addition, for the days between order and shipping date, we will impute the NAs using the median because the values are skewed right.\n",
    "For columns that are skewed but contain negative values, we will do a Yeo-Johnson transformation instead, which is similar to a Box-Cox transformation. Unlike Box-Cox, which is limited to strictly positive data, the Yeo-Johnson transformation can handle zero and negative values, making it a more flexible option for real-world datasets. It applies a power transformation that stabilizes variance and makes the data more normally distributed, improving the performance of many machine learning models. By adjusting the shape of the distribution based on the data's characteristics, Yeo-Johnson ensures that features with a wide or asymmetric spread—including those spanning below zero—can be normalized effectively without the need for arbitrary shifts or imputations.\n",
    "For the target variable, label, we will label encode it turning it from (-1, 0, 1) to (0, 1, 2).\n",
    "To prepare the dataset for modeling, several data cleaning and transformation steps were performed:\n",
    "\n",
    "66 NaNs in diesel_price were imputed using the mean, due to the distribution being normal/symmetrics.\n",
    "\n",
    "5941 NaNs in days_between_order_and_ship were imputed using the median, as the distribution was found to be right-skewed.\n",
    "\n",
    "An invalid entry '91732' found in the customer_state column was corrected to 'CA', as this ZIP code corresponds to a location in California.\n",
    "\n",
    "For numerical features with positive skew and no negative values, a log transformation with an added constant of 1e-6 was applied using a custom function safe_log1p() to reduce skewness and stabilize variance.\n",
    "\n",
    "Applied Yeo-Johnson transformation on skewed numerical features with negatives. Unlike the Box-Cox or log transformation, Yeo-Johnson handles zero and negative values, making it more adaptable for real-world data. It adjusts the shape of the distribution to normalize features effectively without the need for data shifting, which helps improve the performance of various machine learning models.\n",
    "\n",
    "Categorical variables with limited cardinality were one-hot encoded to allow inclusion in machine learning models while preventing high-dimensionality issues.\n",
    "\n",
    "We did not include nominal features with many levels to avoid significantly increasing dimensionality.\n",
    "\n",
    "The shipping_mode feature was ordinally encoded based on a logical ranking: ['Standard Class', 'Second Class', 'First Class', 'Same Day'].\n",
    "\n",
    "All numeric predictors were standardized using StandardScaler() to ensure consistent scaling across features.\n",
    "\n",
    "The target variable label, originally consisting of the values -1, 0, and 1, was label-encoded into 0, 1, and 2 respectively, to make it compatible with scikit-learn's classification algorithms.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [23]:\n",
    "\n",
    "\n",
    "merged_df_copy = merged_df.copy() # Make a copy for merged_df to let XGBoost handle NAs\n",
    "merged_df['store_location_state'] = merged_df['store_location_state'].replace('91732', 'CA')\n",
    "merged_df['diesel_price'] = merged_df['diesel_price'].fillna(merged_df['diesel_price'].mean())\n",
    "merged_df['days_between_order_and_ship'] = merged_df['days_between_order_and_ship'].fillna(merged_df['days_between_order_and_ship'].median())\n",
    "\n",
    "# Custom log to ensure negatives don't cause errors\n",
    "def safe_log1p(X):\n",
    "    return np.log1p(np.nan_to_num(X)) + 1e-6\n",
    "\n",
    "transform_cols = [\n",
    "    'item_discount_amount', 'item_price_before_discount', 'product_retail_price',\n",
    "    'total_sales_per_customer', 'total_sales', 'item_total_after_discount',\n",
    "    'item_profit_margin', 'pre_discount_profit_per_order', 'post_discount_profit_per_order',\n",
    "    'days_between_order_and_ship'\n",
    "]\n",
    "\n",
    "safe_log_cols = [col for col in transform_cols if (merged_df[col] >= 0).all()]\n",
    "yeo_johnson_cols = [col for col in transform_cols if (merged_df[col] < 0).any()]\n",
    "\n",
    "print('Columns to Log:', safe_log_cols)\n",
    "print(\"Columns for Yeo-Johnson Transform:\", yeo_johnson_cols)\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [(\"log\", FunctionTransformer(safe_log1p, feature_names_out='one-to-one'),\n",
    "      safe_log_cols),\n",
    "     (\"yeojohnson\", PowerTransformer(method='yeo-johnson'), yeo_johnson_cols),\n",
    "     (\"ordinal\", OrdinalEncoder(categories=[['Standard Class', 'Second Class', 'First Class', 'Same Day']],\n",
    "                                handle_unknown='use_encoded_value', unknown_value=-1), \n",
    "      ['shipping_mode']),     \n",
    "     (\"onehot\", OneHotEncoder(drop='first'),\n",
    "      ['payment_type', 'customer_purchase_country', 'customer_segment', 'department_name',\n",
    "       'market', 'order_region', 'order_current_status', 'order_dayofweek', 'order_month'])], \n",
    "    remainder='passthrough', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5163ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have selected columns that not only do we believe to be important but also categorical features that do not have many levels to avoid adding dimensionality to the data, as several categorical columns contained hundreds to thousands of levels. Furthermore, these high-level factor variables were redundant in that other features contained similar information while adding less dimensionality from onehot encoding. For example, customer_purchase_city, has 555 levels, whereas customer_purchase_country has only 2 levels and market has 5 levels and still cover similar information.\n",
    "After initial feature selection, we have retained 28 predictors before onehot and ordinal encoding.\n",
    "Now that all the preprocessing is done, we will now perform a 70/30 training-test split stratified on the delivery outcome (label).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a63ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Train/Test Split¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [26]:\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(merged_df['label'])\n",
    "\n",
    "# Select only relevant and not redundant non-ID columns\n",
    "X = merged_df.iloc[:, [0, 1, 2, 6, 8, 12, 13, 14, 15, 22, \n",
    "                       23, 25, 26, 27, 28, 29, 30, 31, 33, \n",
    "                       37, 39, 41, 43, 44, 45, 46, 47, 48]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, train_size=.7, random_state=1, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [27]:\n",
    "\n",
    "\n",
    "# Fit transformer on training data\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Get total number of output features\n",
    "num_transformed_features = len(ct.get_feature_names_out())\n",
    "print(f\"Number of predictors after ColumnTransformer: {num_transformed_features}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf07af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After preprocessing, we will have 84 predictors, while this is much less than if we had kept all columns, we should still be careful about overfitting and regularize when applicable in the models.\n",
    "We can now begin the modeling. The models we will build are:\n",
    "\n",
    "Softmax Regression\n",
    "Multiclass Random Forest\n",
    "Multiclass XGBoost\n",
    "Binary version of the best performing multiclass model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Results¶Model Assumptions¶Now we will check the necessary assumptions for each of the models listed below.\n",
    "\n",
    "Logistic Regression\n",
    "Linearity of Log-Odds\n",
    "Multicollinearity: We will address this by tuning the L2 penalty parameter\n",
    "Outliers: Log-transformation and Yeo-Johnson to help mitigate outliers\n",
    "Independence of observations: True by default, as one order’s delivery outcome doesn't affect another's.\n",
    "\n",
    "\n",
    "Random Forest\n",
    "Independence of observations: True\n",
    "Sufficient data: True, since we have more than 10 times the number of features in rows\n",
    "\n",
    "\n",
    "XGBoost\n",
    "Independence of observations: True\n",
    "Sufficient data: True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [30]:\n",
    "\n",
    "\n",
    "# Linearity of Log-Odds assumption check\n",
    "X_ct = ct.fit_transform(X_train)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_ct)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_scaled, y_train)\n",
    "probs = model.predict_proba(X_scaled)[:, 1]\n",
    "logit = np.log(probs / (1 - probs))\n",
    "\n",
    "feature_names = ct.get_feature_names_out()\n",
    "selected_features = ['item_profit_margin', 'total_sales_per_customer', 'product_retail_price']\n",
    "df_plot = pd.DataFrame({\n",
    "    feat: X_scaled[:, [j for j, name in enumerate(feature_names) if feat in name][0]]\n",
    "    for feat in selected_features\n",
    "})\n",
    "df_plot['logit'] = logit\n",
    "df_plot['label'] = y_train\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"Figure 9: Linearity Check of Logit vs Selected Predictors (Scaled)\", fontsize=14)\n",
    "for ax, feat in zip(axes, selected_features):\n",
    "    sns.scatterplot(data=df_plot, x=feat, y='logit', hue='label', ax=ax, alpha=0.6)\n",
    "    ax.set_title(f\"{feat.replace('_', ' ').title()}\")\n",
    "    ax.set_xlabel(f\"{feat.replace('_', ' ').title()}\")\n",
    "    ax.set_ylabel(\"Logit\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46998df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For brevity, we plotted the logit against only a few continuous predictors as seen in Figure 9, in which there is insufficient visual evidence to meet the linearity of log-odds assumption for logistic regression. Instead, our predictors seem to follow a uniform distribution, suggesting a nonlinear model may be better suited. Therefore our logistic regression model will not be interpretable, but can still be used for prediction which is still suitable for goal, as we are prioritizing prediction (performance) over interpretation. We can expect the softmax regression model to underperform due to many features not being linearly related to the logit of the delivery outcome. However, we can still interpret the random forest and XGBoost models due to having their assumptions met.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Multinomial Classification¶We can now finally begin building our models, starting with logistic regression, followed by random forest, and lastly a boosted model. All three models will incorporate hyperparameter tuning using 5-fold cross-validation, stratified by the delivery outcome.\n",
    "We begin by building multiclass models to predict each delivery outcome. Subsequently, we will binarize the target variable into 'delayed' (1) and 'not delayed' (0), and retrain the best-performing model from the multiclass setting for binary classification before ultimately comparing across all models.\n",
    "Our pipeline will consist of the column transformations specified earlier, followed by a mean imputation step applied to any remaining numeric variables. Although our current dataset has already addressed missing values using manual mean and median imputation, this step is included in the pipeline to ensure robustness. If the model is reused in the future on new data containing missing values, this imputation step will automatically handle them and maintain model stability. Finally, all predictors will be standardized to prepare the data for modeling.\n",
    "Although we care most about capturing as many delays as possible, we used recall_macro as the scoring metric for stratified k-fold cross-validation. By treating all delivery outcomes equally, we prevent the model from achieving high recall merely by predicting delayed deliveries, which represent the majority class. This approach ensures balanced sensitivity across early, on-time, and delayed classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63480d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [33]:\n",
    "\n",
    "\n",
    "def evaluate_model(display_name, y_test, y_pred):\n",
    "    print(f\"\\n{display_name}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "def run_grid_search_pipeline(display_name, pipeline, param_grid, X_train, y_train, X_test, y_test):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1, error_score='raise', scoring='recall_macro')\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"\\n{display_name} Best Params: {grid.best_params_}\")\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    evaluate_model(display_name, y_test, y_pred)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Softmax Regression¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ef2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [35]:\n",
    "\n",
    "\n",
    "lr_pipeline = Pipeline([(\"preprocess\", ct),\n",
    "                        (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "                        (\"scale\", StandardScaler()),\n",
    "                        (\"lr\", LogisticRegression(random_state=1, solver='lbfgs', max_iter=1000, n_jobs=-1))])\n",
    "lr_param_grid = {'lr__C': np.logspace(-4, 4, 50)}\n",
    "best_lr = run_grid_search_pipeline(\"Softmax Regression\", lr_pipeline, lr_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5894dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The best hyperparameter for the softmax regression model was C = 24.42. In multinomial logistic regression, C controls the inverse of regularization strength. A relatively high C indicates that the model performed best with weaker regularization, allowing more flexibility in fitting the class boundaries between the multiple delivery outcomes (early, on-time, delayed). This suggests that the model benefits from retaining more of the signal in the data, possibly due to informative and well-preprocessed features.\n",
    "Softmax Regression performed well in identifying delayed deliveries (Recall: 88.9%) and moderately on early deliveries (Recall: 31.2%), but failed to detect on-time deliveries (Recall: 0.1%), resulting in modest overall accuracy (58.5%) and low macro-average recall (40.1%).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db43443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Random Forest¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b770ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [38]:\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline([(\"preprocess\", ct),\n",
    "                        (\"impute\", SimpleImputer(strategy=\"mean\")),\n",
    "                        (\"scale\", StandardScaler()),\n",
    "                        (\"rf\", RandomForestClassifier(random_state=1, n_jobs=-1))])\n",
    "rf_param_grid = {'rf__n_estimators': [25, 50, 75, 100], 'rf__max_depth': [15, 20, 25], 'rf__min_samples_split': [2, 3, 4]}\n",
    "best_rf = run_grid_search_pipeline(\"Random Forest\", rf_pipeline, rf_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The best-performing Random Forest used 50 trees, a maximum depth of 25, and required at least 4 samples to split a node. This configuration suggests the model benefited from moderately deep trees with slightly more conservative splitting, likely helping to reduce overfitting while still capturing complex patterns in the data.\n",
    "Random Forest achieved strong recall on delayed deliveries (85.1%) and moderate performance on early deliveries (Recall: 41.9%), but performed poorly on on-time deliveries (Recall: 1.9%), resulting in overall accuracy of 59.0% and unbalanced class performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ab860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "XGBoost¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [41]:\n",
    "\n",
    "\n",
    "xgb_pipeline = Pipeline([(\"preprocess\", ct), \n",
    "                         (\"scale\", StandardScaler()), \n",
    "                         (\"xgb\", XGBClassifier(objective='multi:softprob', random_state=1, n_jobs=-1, eval_metric='mlogloss'))])\n",
    "xgb_param_grid = {'xgb__n_estimators': [30, 55, 90, ], 'xgb__max_depth': [4, 5, 6], 'xgb__learning_rate': [0.005, 0.01, 0.05], 'xgb__subsample': [0.7, 0.8, 0.9]}\n",
    "best_xgb = run_grid_search_pipeline(\"XGBoost\", xgb_pipeline, xgb_param_grid, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The best-performing XGBoost model used a low learning rate (0.01), shallow trees (max depth = 4), and fewer boosting rounds (30 estimators) with a high subsample rate (90%). This configuration indicates that the model prioritized conservative, gradual learning with broad data coverage to improve generalization and reduce overfitting.\n",
    "XGBoost showed the most balanced multiclass performance, with strong recall on delayed deliveries (78.9%) and early deliveries (62.0%), while moderately improving on-time recall (6.6%). Given this relative balance, it will serve as the basis for the binary classification comparison.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Binary Classification¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To improve generalizability and mitigate the slight class imbalance observed in the multiclass setting, we now explore a binary classification approach. Instead of predicting three separate delivery outcomes (early, on-time, delayed), we consolidate early and on-time deliveries into a single class labeled as 0 (not delayed), while delayed deliveries are labeled as 1 (delayed). This binarization simplifies the prediction task and focuses model learning on distinguishing between acceptable and problematic outcomes from a business perspective.\n",
    "This approach is particularly valuable because the on-time class was severely underrepresented in predictions, leading to poor recall in the multiclass models. By grouping early and on-time outcomes—both of which are operationally satisfactory—we reduce noise and ambiguity in the target variable, making it easier for the model to learn a clear distinction between satisfactory and unsatisfactory deliveries.\n",
    "Moreover, from a business standpoint, the primary concern is identifying and reducing delayed shipments, as they are most likely to affect customer satisfaction and supply chain efficiency. For this reason, we adjust the scoring metric used during hyperparameter tuning to focus on recall, prioritizing the model's ability to correctly identify as many delayed deliveries as possible, even if it comes at the expense of precision.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [45]:\n",
    "\n",
    "\n",
    "def evaluate_model_binary(display_name, y_test, y_pred):\n",
    "    print(f\"\\n{display_name}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "def run_grid_search_pipeline_binary(display_name, pipeline, param_grid, X_train, y_train, X_test, y_test, scoring='roc_auc'):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=-1, verbose=1, error_score='raise')\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"\\n{display_name} Best Params: {grid.best_params_}\")\n",
    "    y_pred = grid.best_estimator_.predict(X_test)\n",
    "    evaluate_model_binary(display_name, y_test, y_pred)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ede310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "XGBoost (Binary)¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35117418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [47]:\n",
    "\n",
    "\n",
    "y = (merged_df_copy['label'] == 1).astype(int).values\n",
    "X = merged_df_copy.iloc[:, [0, 1, 2, 6, 8, 12, 13, 14, 15, 22, \n",
    "                       23, 25, 26, 27, 28, 29, 30, 31, 33, \n",
    "                       37, 39, 41, 43, 44, 45, 46, 47, 48]]\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y, test_size=.3, train_size=.7, random_state=1, stratify=y)\n",
    "xgb_pipeline = Pipeline([(\"preprocess\", ct), (\"scale\", StandardScaler()), (\"xgb\", XGBClassifier(objective='binary:logistic', random_state=1, n_jobs=-1, eval_metric='logloss'))])\n",
    "xgb_param_grid = {'xgb__n_estimators': [30, 55, 90], 'xgb__max_depth': [4, 6], 'xgb__learning_rate': [0.01, 0.05], 'xgb__subsample': [0.8, 0.9], 'xgb__colsample_bytree': [0.8, 0.9, 1]}\n",
    "best_xgb_binary = run_grid_search_pipeline_binary(\"XGBoost (Binary Classification)\", xgb_pipeline, xgb_param_grid, X_train_binary, y_train_binary, X_test_binary, y_test_binary)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The XGBoost binary classifier achieved an accuracy of 69.0%, with a recall of 81.1% and precision of 70.0% for the delayed class. The macro-averaged recall was 66.8% and macro-averaged precision was 68.6%. The best hyperparameters identified were: colsample_bytree = 1, learning_rate = 0.01, max_depth = 4, n_estimators = 55, and subsample = 0.9.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51137151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Discussion¶We will compare the models using their metrics and confusion matrices, followed by the ROC curve and AUC plot of the best-performing model, and finally the top 30 feature importances—each accompanied by a focused discussion of its findings.\n",
    "Model Comparison¶\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [50]:\n",
    "\n",
    "\n",
    "# Confusion Matrices Display\n",
    "models = [(\"Softmax Regression\", best_lr), (\"Random Forest\", best_rf), (\"XGBoost (Multiclass)\", best_xgb), (\"XGBoost (Binary)\", best_xgb_binary)]\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(4 * len(models), 4))\n",
    "for ax, (name, model) in zip(axes, models):\n",
    "    if \"Binary\" in name:\n",
    "        y_true = y_test_binary; y_pred = model.predict(X_test_binary)\n",
    "    else:\n",
    "        y_true = y_test; y_pred = model.predict(X_test)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap=\"Blues\", ax=ax); ax.set_title(name); ax.grid(False)\n",
    "plt.tight_layout(); plt.subplots_adjust(top=0.8); plt.suptitle('Figure 10: Model Confusion Matrices', fontsize=14); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 2: Model Metrics Comparison¶\n",
    "\n",
    "\n",
    "Model\n",
    "Accuracy\n",
    "Recall\n",
    "Precision\n",
    "Recall (Early)\n",
    "Recall (On‑Time)\n",
    "Recall (Delayed)\n",
    "Recall (Not Delayed)\n",
    "Precision (Early)\n",
    "Precision (On‑Time)\n",
    "Precision (Delayed)\n",
    "Precision (Not Delayed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Softmax Regression\n",
    "0.585\n",
    "0.401\n",
    "0.513\n",
    "0.312\n",
    "0.001\n",
    "0.889\n",
    "—\n",
    "0.419\n",
    "0.500\n",
    "0.619\n",
    "—\n",
    "\n",
    "\n",
    "Random Forest\n",
    "0.590\n",
    "0.430\n",
    "0.439\n",
    "0.419\n",
    "0.019\n",
    "0.851\n",
    "—\n",
    "0.434\n",
    "0.239\n",
    "0.642\n",
    "—\n",
    "\n",
    "\n",
    "XGBoost\n",
    "0.610\n",
    "0.492\n",
    "0.576\n",
    "0.620\n",
    "0.066\n",
    "0.789\n",
    "—\n",
    "0.440\n",
    "0.594\n",
    "0.694\n",
    "—\n",
    "\n",
    "\n",
    "XGBoost (Binary) 🏆\n",
    "0.690\n",
    "0.668\n",
    "0.686\n",
    "—\n",
    "—\n",
    "0.811\n",
    "0.525\n",
    "—\n",
    "—\n",
    "0.700\n",
    "0.671\n",
    "\n",
    "\n",
    "\n",
    "Looking at Table 2 and Figure 10, the XGBoost Binary classifier was the strongest performer for the project's goal: early and accurate detection of delayed deliveries. It achieved the highest recall for delayed shipments (81.1%) and a strong precision (70.0%), making it the most effective model for minimizing undetected delays. Its confusion matrix shows that a large number of true delayed deliveries (class 1) were correctly identified, while false positives for delays remained moderate. This demonstrates the model’s ability to identify most delayed orders—while maintaining respectable precision—enabling proactive mitigation strategies, even at the cost of some false alarms. Additionally, its overall accuracy (69.0%) and balanced performance confirm its practical utility in a real-world logistics environment.\n",
    "In the multiclass setting, the standard XGBoost model demonstrated the best overall multiclass performance, with the highest accuracy (61.0%) and the most balanced combination of recall (49.2%) and precision (57.6%) across the three delivery classes. As seen in its confusion matrix, the model correctly identified a majority of delayed shipments and performed well in detecting early deliveries (recall: 62.0%), but again struggled with identifying on-time deliveries, achieving only 6.6% recall. A large number of on-time deliveries were misclassified as delayed or early. Despite this limitation, the model’s strength in distinguishing extreme outcomes—early versus delayed—could still provide strategic value in scenarios where proactive planning hinges more on detecting outliers than exact timing.\n",
    "The Random Forest model showed moderate performance, with delay recall at 85.1%, slightly outperforming XGBoost in that aspect. However, its early (41.9%) and on-time (1.9%) recall were much lower. The confusion matrix for this model illustrates the severe misclassification of on-time deliveries, which are almost entirely absorbed into the delayed class. Precision across all classes is inconsistent, suggesting that while the model effectively identifies late deliveries, it lacks nuance in differentiating early and on-time cases. This limits its usefulness in a multiclass classification context where more granular insights are needed.\n",
    "The Softmax Regression model performed weakest overall, with the lowest accuracy (58.5%) and the lowest macro recall (40.1%). It particularly struggled with on-time recall, achieving a mere 0.1%, essentially failing to detect this class. As shown in its confusion matrix, most on-time deliveries were predicted as delayed, leading to significant class imbalance in the predictions. While its delay recall (88.9%) is high and precision on early deliveries (41.9%) is relatively strong, its inability to handle all three classes effectively underscores the limitations of using a simpler linear model for this type of problem.\n",
    "In summary, the confusion matrices (Figure 10) visually support and reinforce the performance metrics shown in Table 2, highlighting the per-class strengths and trade-offs of each model. While the multiclass XGBoost model demonstrated better balance overall, its poor on-time detection detracts from its utility in real-world settings. The binary XGBoost model, by simplifying the target to delayed versus not delayed, successfully maximized recall on the most important class and maintained strong precision—making it best suited for business-critical delay prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f98219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ROC Curve and AUC¶Now that we have chosen the final model (XGBoost binary model), we will look deeper into the ROC curve and the AUC to get a better understanding of it's performance before proceeding to the feature analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fa7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [53]:\n",
    "\n",
    "\n",
    "# ROC Curve Plot w/ AUC\n",
    "y_proba = best_xgb_binary.predict_proba(X_test_binary)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_binary, y_proba)\n",
    "auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6)); plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Baseline'); plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title('Figure 11: ROC Curve of XGBoost Binary Model', fontsize=14)\n",
    "plt.legend(loc='lower right'); plt.grid(True); plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Figure 11 shows the ROC curve for the binary XGBoost model, used to predict whether a delivery will be delayed. The model achieves an AUC (Area Under the Curve) of 0.777, indicating good overall classification performance. An AUC closer to 1.0 reflects a stronger ability to distinguish between delayed and non-delayed shipments, while 0.5 would represent random chance.\n",
    "The ROC curve rises steeply at the beginning, showing that the model captures a high true positive rate (recall) at relatively low false positive rates. This is valuable in the logistics context, where correctly identifying delayed shipments is more critical than occasionally flagging a non-delayed one. The model is therefore effective at detecting most delays early without a high cost of false alarms.\n",
    "The diagonal line represents a random classifier. The XGBoost curve consistently stays above this baseline, confirming meaningful predictive skill. Overall, the model balances sensitivity and specificity well and is a strong candidate for real-world deployment where early detection of delays is essential for proactive operational response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Feature Analysis¶Now that we have chosen the final model (XGBoost binary model), we will perform a feature analysis on the most important predictors according to the model. This will give us a better understanding of which features are important and which can be safely removed with no loss in model performance. Only the top 30 features will be displayed for brevity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "In [56]:\n",
    "\n",
    "\n",
    "# Get Feature Names and Importances\n",
    "feature_names = ct.get_feature_names_out()\n",
    "xgb_model = best_xgb_binary.named_steps['xgb']\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Sort by top 30 importances\n",
    "indices = np.argsort(importances)[::-1]\n",
    "sorted_features = np.array(feature_names)[indices]\n",
    "sorted_importances = importances[indices]\n",
    "top_n = 30\n",
    "top_features = sorted_features[:top_n]\n",
    "top_importances = sorted_importances[:top_n]\n",
    "top_features_reversed = top_features[:top_n][::-1]\n",
    "top_importances_reversed = top_importances[:top_n][::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.set(style=\"whitegrid\")\n",
    "bars = plt.barh(range(top_n), top_importances_reversed, align='center', color=sns.color_palette(\"Blues\", top_n))\n",
    "plt.yticks(range(top_n), top_features_reversed, fontsize=10)\n",
    "plt.xlabel(\"Importance Score\", fontsize=12)\n",
    "plt.title(\"Figure 12: Top 30 Feature Importances from XGBoost (Binary Classification)\", fontsize=14)\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{top_importances_reversed[i]:.3f}\", va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd756f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The feature importance plot shown in Figure 12 reveals that shipping_mode overwhelmingly dominates the model, accounting for 66% of the total importance across all 84 predictors. This is consistent with domain expectations—shipping services with wider delivery windows (e.g., Standard Class) are naturally less likely to be flagged as delayed compared to express modes with stricter timing.\n",
    "Furthermore, diesel_price, sourced from the U.S. Energy Information Administration, ranks as the 8th most important feature, indicating that macro-level transportation cost fluctuations contributed modestly to model performance—more so than most other variables.\n",
    "Other moderately important features include days_between_order_and_ship, invalid_date_flow, and longitude, each contributing around 2–3%. These features likely reflect latent temporal or geographic delivery dynamics, such as inefficient processing gaps or regional shipping differences.\n",
    "A long tail of low-importance features follows, including many one-hot encoded indicators like customer_segment_Home Office and payment_type_PAYMENT. While individually minor, their presence suggests the model captures some behavioral or operational patterns.\n",
    "Interestingly, most transactional features—including profit margins, item discounts, and sales totals—contributed very little to the model. This emphasizes that operational metadata (e.g., logistics details, timing, and geography) is more predictive of delivery delays than financial or sales-related variables.\n",
    "Additionally, multiple order_region_* features show small but roughly equal importance, suggesting mild geographic variation without any single region driving the prediction.\n",
    "With 84 predictors in total, the steep drop-off in importance after the top 2–3 features highlights strong potential for feature reduction. Strategies such as removing weak predictors, consolidating sparse categorical levels, or using PCA on less informative variables could simplify the model without significant loss in performance.\n",
    "Lastly, we can also replace many of these unimportant features with columns we decided to remove due to dimensionality concerns and belief they would be redundant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e9b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Next Steps¶While the models developed in this project demonstrated moderate to strong predictive performance—particularly the binary XGBoost classifier—there are several avenues for further refinement and enhancement based on the feature importance findings.\n",
    "First, feature selection and engineering could be optimized further. The feature importance plot revealed that only a small subset of predictors—most notably shipping_mode—accounted for the majority of the model’s predictive power. This suggests strong potential for feature pruning, where low-importance features can be removed to streamline the model and reduce complexity without sacrificing accuracy. At the same time, new features may be engineered to capture additional structure, such as interaction terms or lag-based temporal variables (e.g., rolling averages of delivery performance).\n",
    "Additionally, while diesel price contributed modestly as the 8th most important predictor, its impact opens the door to incorporating other fuel-based macroeconomic indicators. In particular, jet fuel prices could be explored in future iterations, as Figures 4 and 5 suggest that international deliveries are common. Including jet fuel trends could provide stronger signal for air-based shipments, especially in overseas contexts.\n",
    "Another promising direction involves reducing the number of levels in categorical variables. Several categorical features—like customer segments, order regions, and store locations—appeared with marginal yet redundant influence. Consolidating rare or overlapping levels into broader categories could reduce dimensionality and improve generalization, especially for one-hot encoded variables that expand the feature space unnecessarily.\n",
    "Probability calibration may also improve performance. Techniques such as Platt scaling or isotonic regression can make predicted probabilities more reliable and actionable. This is especially relevant in the binary classification setting, where recall is prioritized to capture as many delayed deliveries as possible.\n",
    "Lastly, incorporating unsupervised learning techniques could add predictive value. For example, applying K-Means clustering to PCA-reduced features may uncover latent structures in the data, such as underlying order patterns or customer segments. The resulting cluster labels can then be added as categorical features, helping the model capture complex, nonlinear relationships that may not be apparent through raw variables alone.\n",
    "Overall, the final model performs well, but additional refinements in feature engineering, dimensionality reduction, and probability calibration have the potential to further improve its predictive accuracy and practical impact.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Conclusion¶This machine learning project aimed to predict delivery outcomes—early, on-time, or delayed—using real-world logistics data, enriched with external diesel price trends. Through feature engineering and model tuning, multiple classification algorithms were evaluated. Among them, the XGBoost Binary model emerged as the most reliable, offering the highest recall on delayed deliveries. This capability is especially critical in logistics, where failing to anticipate delays can lead to operational disruptions and customer dissatisfaction.\n",
    "While models like Random Forest and multiclass XGBoost offered nuanced classifications, they struggled with on-time delivery detection and required more complexity for marginal gains. Simpler models like Softmax Regression lacked the capacity to generalize effectively across classes.\n",
    "Ultimately, this work demonstrates that predictive modeling—when properly tuned and evaluated—can offer real, actionable value in managing supply chain reliability. The selected model enables smarter decision-making, anticipatory logistics planning, and improved customer experience. Furthermore, with the proper additions specified in above such as calibration, feature engineering clusters via K-Means, and further feature selection, the final model can likely be improved even more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "References¶\n",
    "Gabellini, M., Civolani, L., Calabrese, F., & Bortolini, M. (2024). A Deep Learning Approach to Predict Supply Chain Delivery Delay Risk Based on Macroeconomic Indicators: A Case Study in the Automotive Sector. Applied Sciences, 14, 4688. https://doi.org/10.3390/app14114688\n",
    "\n",
    "Ivanov, D., & Dolgui, A. (2020). A Digital Supply Chain Twin for Managing the Disruption Risks and Resilience in the Era of Industry 4.0. Production Planning & Control, 32, 775–788. https://doi.org/10.1080/09537287.2020.1768450\n",
    "\n",
    "Pazoki, M., Samarghandi, H., & Behroozi, M. (2023, August 1). Increasing Supply Chain Resiliency Through Equilibrium Pricing and Stipulating Transportation Quota Regulation. https://doi.org/10.48550/arXiv.2308.00681\n",
    "\n",
    "Rokoss, A., Syberg, M., Tomidei, L., Hülsing, C., Deuse, J., & Schmidt, M. (2024). Case Study on Delivery Time Determination Using a Machine Learning Approach in Small Batch Production Companies. Journal of Intelligent Manufacturing, 35, 3937–3958. https://doi.org/10.1007/s10845-023-02290-2\n",
    "\n",
    "\n",
    "Data Sources¶\n",
    "Administration, U. E. (2025, March 17). Weekly Retail Gasoline and Diesel Prices. United States. Retrieved March 23, 2025, from https://www.eia.gov/dnav/pet/pet_pri_gnd_dcus_nus_w.htm\n",
    "\n",
    "Kamboj, P. (2024, September 2). Logistics Supply Chain Real World Data. Retrieved from https://www.kaggle.com/datasets/pushpitkamboj/logistics-data-containing-real-world-data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
